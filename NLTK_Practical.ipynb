{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8afbb9c",
   "metadata": {},
   "source": [
    "Natural Language processing is the field that focous on making natural human language usable by computer program. NLTK , or the Natural Language toolkit is a python package that can use for NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f6d391f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk==3.5 in c:\\users\\aswin\\anaconda3\\lib\\site-packages (3.5)\n",
      "Requirement already satisfied: joblib in c:\\users\\aswin\\anaconda3\\lib\\site-packages (from nltk==3.5) (1.2.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\aswin\\anaconda3\\lib\\site-packages (from nltk==3.5) (4.43.0)\n",
      "Requirement already satisfied: regex in c:\\users\\aswin\\anaconda3\\lib\\site-packages (from nltk==3.5) (2022.7.9)\n",
      "Requirement already satisfied: click in c:\\users\\aswin\\anaconda3\\lib\\site-packages (from nltk==3.5) (8.0.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\aswin\\anaconda3\\lib\\site-packages (from click->nltk==3.5) (0.4.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -andas (c:\\users\\aswin\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -andas (c:\\users\\aswin\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -andas (c:\\users\\aswin\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -andas (c:\\users\\aswin\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -andas (c:\\users\\aswin\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -andas (c:\\users\\aswin\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install nltk==3.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030f5e51",
   "metadata": {},
   "source": [
    "## Tokenizing \n",
    "\n",
    "This will allow to work with the smaller pieces of the text that are still relatively coherent andd meaniningful even outside of the context of the rest of the text.\n",
    "\n",
    "* Tokenizing by word: Words are like the atoms of natural language. They’re the smallest unit of meaning that still makes sense on its own. Tokenizing your text by word allows you to identify words that come up particularly often.\n",
    "\n",
    "* Tokenizing by sentence: When you tokenize by sentence, you can analyze how those words relate to one another and see more context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2c1146e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccfb22c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"\\n Muad'Dib learned rapidly because his first training was in how to learn.\",\n",
       " 'And the first lesson of all was the basic trust that he could learn.',\n",
       " \"It's shocking to find how many people do not believe they can learn,\\n and how many more believe learning to be difficult.\"]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = \"\"\"\n",
    " Muad'Dib learned rapidly because his first training was in how to learn.\n",
    " And the first lesson of all was the basic trust that he could learn.\n",
    " It's shocking to find how many people do not believe they can learn,\n",
    " and how many more believe learning to be difficult.\n",
    "\"\"\"\n",
    "\n",
    "sent_tokenize(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3df1c49",
   "metadata": {},
   "source": [
    "# After tokenize the sentence from the pragraph it make three sentence\n",
    "\n",
    "1.Muad'Dib learned rapidly because his first training was in how to learn.\n",
    "\n",
    "2.And the first lesson of all was the basic trust that he could learn.\n",
    "\n",
    "3.It's shocking to find how many people do not believe they can learn,\\n and how many more believe learning to be difficult."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa03937f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Muad'Dib\",\n",
       " 'learned',\n",
       " 'rapidly',\n",
       " 'because',\n",
       " 'his',\n",
       " 'first',\n",
       " 'training',\n",
       " 'was',\n",
       " 'in',\n",
       " 'how',\n",
       " 'to',\n",
       " 'learn',\n",
       " '.',\n",
       " 'And',\n",
       " 'the',\n",
       " 'first',\n",
       " 'lesson',\n",
       " 'of',\n",
       " 'all',\n",
       " 'was',\n",
       " 'the',\n",
       " 'basic',\n",
       " 'trust',\n",
       " 'that',\n",
       " 'he',\n",
       " 'could',\n",
       " 'learn',\n",
       " '.',\n",
       " 'It',\n",
       " \"'s\",\n",
       " 'shocking',\n",
       " 'to',\n",
       " 'find',\n",
       " 'how',\n",
       " 'many',\n",
       " 'people',\n",
       " 'do',\n",
       " 'not',\n",
       " 'believe',\n",
       " 'they',\n",
       " 'can',\n",
       " 'learn',\n",
       " ',',\n",
       " 'and',\n",
       " 'how',\n",
       " 'many',\n",
       " 'more',\n",
       " 'believe',\n",
       " 'learning',\n",
       " 'to',\n",
       " 'be',\n",
       " 'difficult',\n",
       " '.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Word tokennization\n",
    "\n",
    "word_tokenize(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c24ba3",
   "metadata": {},
   "source": [
    "##  Filtering the stoping word\n",
    "\n",
    "Stop words are words that you want to ignore, so you filter them out of your text when you’re processing it. Very common words like 'in', 'is', and 'an' are often used as stop words since they don’t add a lot of meaning to a text in and of themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62242b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f1a965a",
   "metadata": {},
   "outputs": [],
   "source": [
    "worf_quote = \"Sir, I protest. I am not a merry man!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14b40033",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sir', ',', 'I', 'protest', '.', 'I', 'am', 'not', 'a', 'merry', 'man', '!']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_in_quote = word_tokenize(worf_quote)\n",
    "word_in_quote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bcc30249",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7d6f527",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_list = []\n",
    "for word in word_in_quote:\n",
    "    if word.casefold() not in stop_words:\n",
    "        filtered_list.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6297925e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sir', ',', 'protest', '.', 'merry', 'man', '!']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8336029",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "\n",
    "Stemming is a text processing task in which you reduce words to their root, which is the core part of a word. For example, the words “helping” and “helper” share the root “help.” "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2ef8503",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0faf3e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f023f91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_for_stemming = \"\"\"\n",
    " The crew of the USS Discovery discovered many discoveries.\n",
    " Discovering is what explorers do.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33817593",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = word_tokenize(string_for_stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd81a858",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'crew',\n",
       " 'of',\n",
       " 'the',\n",
       " 'USS',\n",
       " 'Discovery',\n",
       " 'discovered',\n",
       " 'many',\n",
       " 'discoveries',\n",
       " '.',\n",
       " 'Discovering',\n",
       " 'is',\n",
       " 'what',\n",
       " 'explorers',\n",
       " 'do',\n",
       " '.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a863005b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stremmed_word = []\n",
    "\n",
    "for word in words:\n",
    "    stremmed_word.append(stemmer.stem(word))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4992b1e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'crew',\n",
       " 'of',\n",
       " 'the',\n",
       " 'uss',\n",
       " 'discoveri',\n",
       " 'discov',\n",
       " 'mani',\n",
       " 'discoveri',\n",
       " '.',\n",
       " 'discov',\n",
       " 'is',\n",
       " 'what',\n",
       " 'explor',\n",
       " 'do',\n",
       " '.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stremmed_word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c53077",
   "metadata": {},
   "source": [
    "## Tagging Parts of Speech\n",
    "\n",
    "Part of speech is a grammatical term that deals with the roles words play when you use them together in sentences. Tagging parts of speech, or POS tagging, is the task of labeling the words in your text according to their part of speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd7529f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagan_quote = \"\"\"\n",
    "If you wish to make an apple pie from scratch,\n",
    " you must first invent the universe.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c995c661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('If', 'IN'),\n",
       " ('you', 'PRP'),\n",
       " ('wish', 'VBP'),\n",
       " ('to', 'TO'),\n",
       " ('make', 'VB'),\n",
       " ('an', 'DT'),\n",
       " ('apple', 'NN'),\n",
       " ('pie', 'NN'),\n",
       " ('from', 'IN'),\n",
       " ('scratch', 'NN'),\n",
       " (',', ','),\n",
       " ('you', 'PRP'),\n",
       " ('must', 'MD'),\n",
       " ('first', 'VB'),\n",
       " ('invent', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('universe', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "words  = word_tokenize(sagan_quote)\n",
    "nltk.pos_tag(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae24b10",
   "metadata": {},
   "source": [
    "## Lemmantizing\n",
    "\n",
    "Now that you’re up to speed on parts of speech, you can circle back to lemmatizing. Like stemming, lemmatizing reduces words to their core meaning, but it will give you a complete English word that makes sense on its own instead of just a fragment of a word like 'discoveri'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "44f6854f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7162de71",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4aea02a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_for_lemmatizing = \"The friends of DeSoto love scarves.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2c838d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = word_tokenize(string_for_lemmatizing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "df064377",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "35f61cdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'friend', 'of', 'DeSoto', 'love', 'scarf', '.']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aef794c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bad'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"worst\", pos=\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7ce21916",
   "metadata": {},
   "outputs": [],
   "source": [
    "quote = \"\"\"\n",
    " Men like Schiaparelli watched the red planet—it is odd, by-the-bye, that\n",
    " for countless centuries Mars has been the star of war—but failed to\n",
    " interpret the fluctuating appearances of the markings they mapped so well.\n",
    " All that time the Martians must have been getting ready.\n",
    "\n",
    " During the opposition of 1894 a great light was seen on the illuminated\n",
    " part of the disk, first at the Lick Observatory, then by Perrotin of Nice,\n",
    " and then by other observers. English readers heard of it first in the\n",
    " issue of Nature dated August 2.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d35e83fe",
   "metadata": {},
   "outputs": [],
   "source": [
    " def extract_ne(quote):\n",
    "        words = word_tokenize(quote)\n",
    "        tags = nltk.pos_tag(words)\n",
    "        tree = nltk.ne_chunk(tags, binary=True)\n",
    "        return set(\n",
    "        \" \".join(i[0] for i in t)\n",
    "         for t in tree\n",
    "         if hasattr(t, \"label\") and t.label() == \"NE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5b051c0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Lick Observatory', 'Mars', 'Nature', 'Perrotin', 'Schiaparelli'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_ne(quote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e35fc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
